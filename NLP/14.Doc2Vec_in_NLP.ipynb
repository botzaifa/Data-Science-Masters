{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing neccessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Doc2Vec is used for creating document embeddings. It captures the context of entire documents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize into sentences and words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences: [['doc2vec', 'is', 'used', 'for', 'creating', 'document', 'embeddings', '.'], ['it', 'captures', 'the', 'context', 'of', 'entire', 'documents', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized sentences:\", tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tagged documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=words, tags=[str(idx)]) for idx, words in enumerate(tokenized_sentences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Doc2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, dm=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec model trained successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Doc2Vec model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer document vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = model.infer_vector(word_tokenize(\"Doc2Vec is a powerful tool for document embeddings.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred document vector: [-2.7706267e-03  1.3429527e-03  1.5869472e-03 -2.4584713e-03\n",
      " -2.3472891e-03  5.3375843e-04 -3.9743059e-03  5.2359996e-05\n",
      "  2.9550598e-03 -2.7604417e-03 -3.7889674e-03 -1.9334015e-03\n",
      "  4.2413096e-03  2.0465918e-03  1.3609939e-03 -4.8937523e-03\n",
      "  8.2520867e-04  5.1127811e-04  6.9515430e-04 -2.3296105e-03\n",
      " -8.0556283e-04 -4.4799200e-03  2.2648200e-03 -4.9373851e-04\n",
      "  5.2303501e-04 -2.0024767e-03 -5.0389739e-03 -3.6044037e-03\n",
      "  1.2511757e-03 -1.3600584e-03  7.2938367e-04  1.1363412e-03\n",
      "  4.0583215e-03  1.4998398e-03  2.0337615e-03  2.4031831e-03\n",
      " -3.7486546e-03  3.3169743e-03 -3.9227023e-03 -2.2029560e-03\n",
      " -3.6247552e-04  1.9469992e-03 -6.3266495e-04  3.0458376e-03\n",
      "  3.0886731e-03 -2.9660142e-03 -8.6042535e-04 -4.4005495e-03\n",
      "  1.7403174e-03  3.7389309e-03  1.0981503e-03 -2.7740896e-03\n",
      "  2.3899253e-03  2.3515951e-03  6.1562960e-04  4.3117297e-03\n",
      "  3.4102448e-03  6.6092581e-04  2.1585708e-03  2.7102887e-04\n",
      " -3.7000000e-03  1.4416573e-03  3.1770305e-03 -1.2749948e-03\n",
      " -9.5629616e-04 -2.8144841e-03  4.0005092e-03  4.0134951e-03\n",
      " -3.2191367e-03  5.3123524e-04 -3.1803516e-03  4.4199550e-03\n",
      "  4.3869242e-03  5.0579209e-04 -3.4824240e-03 -6.0194515e-04\n",
      "  6.3310744e-04  2.0598574e-03 -4.8479676e-04  2.9389483e-03\n",
      "  4.3750745e-03  4.4160706e-04 -2.0463872e-03 -6.8085681e-04\n",
      "  2.8584301e-03  4.5203592e-04 -3.8726144e-03 -4.3231002e-03\n",
      "  2.3141846e-03  4.4976093e-04 -4.0506385e-04 -6.1330475e-06\n",
      " -2.9078141e-04 -3.2452403e-03 -1.4471442e-03  4.5905891e-04\n",
      " -3.5384719e-03  1.5192823e-03 -2.4654754e-03  1.9629812e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferred document vector:\", doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
