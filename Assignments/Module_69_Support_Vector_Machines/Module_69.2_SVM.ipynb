{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment (7th April) : Support Vector Machines - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms? \n",
    "\n",
    "**ANS**: In machine learning, polynomial functions are used as kernel functions in SVMs to map data into a higher-dimensional space where it can be linearly separated. The polynomial kernel is one type of kernel function that enables non-linear classification by transforming the feature space to capture complex relationships. It‚Äôs defined as:\n",
    "\n",
    "- `ùêæ(ùë•ùëñ,ùë•ùëó)=(ùë•ùëñ‚ãÖùë•ùëó+ùëê)^ùëë`\n",
    " \n",
    "where:\n",
    "- `ùë•ùëñ and ùë•ùëó` are feature vectors,\n",
    "- `c` is a constant that controls the influence of higher-degree terms, and\n",
    "- `d` is the degree of the polynomial.\n",
    "\n",
    "By using a polynomial kernel, SVM can fit non-linear boundaries without explicitly computing transformations for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with polynomial kernel: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2] \n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with polynomial kernel: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "**ANS**: In `Support Vector Regression (SVR)`, the `epsilon (ùúñ) parameter` defines a margin of tolerance in which no penalty is given to errors. As ùúñ increases:\n",
    "\n",
    "- The number of support vectors decreases because more points can lie within the ùúñ-margin without penalty.\n",
    "- This can lead to a simpler model with potentially reduced variance but might miss finer details.\n",
    "\n",
    "When ùúñ is small, more data points fall outside the margin, increasing the number of support vectors and potentially improving the model's ability to capture complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "**ANS**:\n",
    "1. `Kernel Function`: Determines the transformation of data into a higher-dimensional space. \n",
    "\n",
    "- Linear: Best for linearly separable data. \n",
    "- Polynomial/RBF: Suitable for non-linear data. Polynomial for specific degree-based patterns; RBF for general non-linearity.\n",
    "\n",
    "2. `C Parameter`: Controls the penalty for errors. \n",
    "\n",
    "- High C: The model tries to minimize error with less tolerance, leading to a lower bias but higher variance. \n",
    "- Low C: The model allows more error, leading to a simpler, more generalized model with higher bias but reduced variance.\n",
    "\n",
    "3. `Epsilon Parameter (œµ)`: Sets the tolerance for error in SVR.\n",
    "\n",
    "- High œµ: Creates a larger margin, resulting in fewer support vectors and a simpler model.\n",
    "- Low œµ: Reduces the margin, making the model capture more variations in data, which can improve performance for noisy datasets.\n",
    "\n",
    "4. `Gamma Parameter`: Controls the influence of individual points.\n",
    "\n",
    "- High ùõæ: Points have close influence, capturing intricate patterns and possibly overfitting.\n",
    "- Low ùõæ: Influence is spread out, producing a smoother, more generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataset. \n",
    "- Split the dataset into training and testing set.\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization).\n",
    "- Create an instance of the SVC classifier and train it on the training data.\n",
    "- Use the trained classifier to predict the labels of the testing data.\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performanc)\n",
    "- Train the tuned classifier on the entire dataset.\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report before tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Best parameters found by GridSearchCV: {'C': 0.1, 'gamma': 1, 'kernel': 'poly'}\n",
      "Model saved to svc_tuned_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# 1.a Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1.bLoad the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 4. Initialize and train the SVC classifier\n",
    "svc = SVC(kernel='rbf', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict the labels for the test set\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the classifier performance\n",
    "print(\"Classification Report before tuning:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(random_state=42), param_grid, refit=True, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 8. Best parameters from grid search\n",
    "print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
    "\n",
    "# 9. Train the tuned classifier on the entire dataset\n",
    "svc_tuned = grid_search.best_estimator_\n",
    "svc_tuned.fit(X, y)\n",
    "\n",
    "# 10. Save the trained classifier to a file\n",
    "joblib.dump(svc_tuned, 'svc_tuned_model.pkl')\n",
    "print(\"Model saved to svc_tuned_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
