{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment (4th April) : Decision Trees - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
    "\n",
    "**ANS**: A Decision tree classifier splits data into branches based on feature values to classify observations. At each node, the model selects a feature that best divides the data according to a criterion (e.g., Gini impurity or information gain). The tree grows by repeatedly partitioning the dataset until it reaches stopping criteria, such as a maximum depth or a minimum number of samples per leaf, creating “leaves” that represent the final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
    "\n",
    "**ANS**: The steps as follows:\n",
    "1. `Calculate Split Criteria`: At each node, the algorithm evaluates possible splits by calculating a criterion (e.g., Gini impurity or entropy) for each feature.\n",
    "2. `Select Optimal Split`: The split with the best score (lowest Gini impurity or highest information gain) is chosen, dividing data at the node.\n",
    "3. `Recursively Split`: The process continues recursively, partitioning each subset until the stopping condition is met.\n",
    "4. `Assign Classes`: At the leaf nodes, the class with the majority of instances is assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "\n",
    "**ANS**: For binary classification, the tree splits data into two classes (e.g., “yes” or “no”) by partitioning on the feature that best separates the classes at each node. The process continues until the tree reaches a maximum depth, leaf purity, or other stopping criteria. Each leaf node will then predict one of the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.\n",
    "\n",
    "**ANS**: In geometric terms, a decision tree divides the feature space into rectangular regions. Each split defines a boundary, segmenting the feature space by creating partitions that isolate instances of different classes. Predictions are made by determining which region a new instance falls into and assigning it the majority class of that region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\n",
    "\n",
    "**ANS**: A confusion matrix is a table summarizing the performance of a classification model by showing the counts of:\n",
    "\n",
    "- `True Positives (TP)`: Correctly classified positive instances.\n",
    "- `True Negatives (TN)`: Correctly classified negative instances.\n",
    "- `False Positives (FP)`: Incorrectly classified positive instances.\n",
    "- `False Negatives (FN)`: Incorrectly classified negative instances.\n",
    "\n",
    "The confusion matrix helps calculate metrics such as accuracy, precision, recall, and F1 score, providing insights into model performance and specific types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.\n",
    "\n",
    "**ANS**: Suppose a binary classification model yields the following confusion matrix:\n",
    "\n",
    "| Actual | Predicted Positive\t| Predicted Negative |\n",
    "|---------|--------|--------|\n",
    "| Actual Positive | TP = 50 | FN = 10 |\n",
    "| Actual Negative | FP = 5 | TN = 35 | \n",
    "\n",
    "\n",
    "- `Precision`: TP / (TP + FP) = 50 / (50+5) = 0.91\n",
    "\n",
    "\n",
    "- `Recall`:  TP / (TP + FN) = 50 / (50+10) = 0.83\n",
    "\n",
    "\n",
    "<p>\n",
    "`F1 Score`: 2 × (Precision × Recall) / (Precision + Recall)  = 2 × (0.91×0.83) / (0.91+0.83) = 0.87\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\n",
    "\n",
    "**ANS**: Choosing the right metric is crucial because it directly impacts the understanding of model performance. Depending on the problem:\n",
    "\n",
    "- `Precision` is crucial when false positives have a high cost (e.g., in spam detection).\n",
    "- `Recall` is more important when false negatives are costly (e.g., medical diagnoses).\n",
    "- `F1 Score` is useful in cases where a balance between precision and recall is needed. Assessing the problem context and deciding the consequences of false positives vs. false negatives help in selecting the best evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Provide an example of a classification problem where precision is the most important metric, and explain why.\n",
    "\n",
    "**ANS**: In `Email Spam Detection`, precision is vital because misclassifying a legitimate email as spam (a false positive) can lead to important emails being missed. Thus, maximizing precision reduces the risk of misclassifying non-spam emails as spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. Provide an example of a classification problem where recall is the most important metric, and explain why.\n",
    "\n",
    "**ANS**: In `Cancer Detection`, recall is essential because failing to identify a true positive (a cancer case) can have severe consequences. Maximizing recall ensures that as many cancer cases as possible are detected, even if it means including some false positives for further screening."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
