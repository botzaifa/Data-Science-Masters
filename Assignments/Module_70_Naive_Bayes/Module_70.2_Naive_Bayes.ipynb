{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment (10th April) : Naive Bayes - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "**ANS:** Let's define the following probabilities based on the information given:\n",
    "\n",
    "- P(Insurance)=0.70: Probability that an employee uses the health insurance plan.\n",
    "- P(Smoker∣Insurance)=0.40: Probability that an employee is a smoker given that they use the insurance.\n",
    "\n",
    "Since we want to find the probability of an employee being a smoker given that they use the insurance, we can directly use:\n",
    "\n",
    "- P(Smoker∣Insurance)=0.40\n",
    "\n",
    "Thus, the probability that an employee is a smoker, given that they use the health insurance plan, is `0.40 or 40%`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "**ANS:** \n",
    "\n",
    "- `Bernoulli Naive Bayes`: This classifier is designed for binary/boolean features (e.g., word presence or absence in text data). It considers each feature to be binary, indicating the occurrence or non-occurrence of a feature.\n",
    "\n",
    "- `Multinomial Naive Bayes`: This classifier is suited for multinomially distributed data, commonly used for text data with term frequencies or counts (e.g., word counts in documents). It captures the frequency or counts of features and is better for tasks involving frequency-based data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "**ANS:** Bernoulli Naive Bayes does not have a specific way to handle missing values directly. Usually, missing values need to be preprocessed by:\n",
    "\n",
    "- Imputing missing values with `zeros` (assuming the feature absence).\n",
    "- Replacing with the `mean or mode` of the feature.\n",
    "- Using `feature engineering` techniques to handle missing values before training the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "**ANS:** `Yes`, Gaussian Naive Bayes can be used for multi-class classification. In this case, it calculates the conditional probability of each class by assuming each class is a Gaussian distribution, and it assigns the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "\n",
    "- Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "- Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "- Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1 score\n",
    "\n",
    "- Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "- Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "**ANS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BernoulliNB Results:\n",
      "Accuracy: 0.8857\n",
      "Precision: 0.8855\n",
      "Recall: 0.8158\n",
      "F1: 0.8490\n",
      "\n",
      "MultinomialNB Results:\n",
      "Accuracy: 0.7903\n",
      "Precision: 0.7407\n",
      "Recall: 0.7215\n",
      "F1: 0.7306\n",
      "\n",
      "GaussianNB Results:\n",
      "Accuracy: 0.8203\n",
      "Precision: 0.6989\n",
      "Recall: 0.9575\n",
      "F1: 0.8078\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"spambase.data\", header=None)  # Adjust path as necessary\n",
    "X = data.iloc[:, :-1]  # Features\n",
    "y = data.iloc[:, -1]   # Labels (spam or not)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"GaussianNB\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate each classifier\n",
    "results = {}\n",
    "for clf_name, clf in classifiers.items():\n",
    "    scores = {}\n",
    "    for metric_name, metric in scoring_metrics.items():\n",
    "        cv_scores = cross_val_score(clf, X, y, cv=kfold, scoring=metric)\n",
    "        scores[metric_name] = cv_scores.mean()\n",
    "    results[clf_name] = scores\n",
    "\n",
    "# Display results\n",
    "for clf_name, scores in results.items():\n",
    "    print(f\"\\n{clf_name} Results:\")\n",
    "    for metric_name, score in scores.items():\n",
    "        print(f\"{metric_name.capitalize()}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "\n",
    "- `Best-performing model`: Likely the Multinomial Naive Bayes, as it aligns well with text data having word frequencies.\n",
    "\n",
    "- `Limitations`: Naive Bayes assumes feature independence, which can limit its performance if there are correlations between features (words) in text data.\n",
    "  \n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "- `Summary`: Multinomial Naive Bayes is likely most suitable for this type of dataset.\n",
    "- `Suggestions for future work`: Consider experimenting with feature engineering (e.g., n-grams, TF-IDF) or applying other algorithms like SVM or Decision Trees if Naive Bayes underperforms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
