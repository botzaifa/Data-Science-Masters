{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea223b0",
   "metadata": {},
   "source": [
    "# Assignment - 20 (WebScraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f3c94",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6f7d5",
   "metadata": {},
   "source": [
    "ANS: Web scraping is the process of extracting data from websites or web pages automatically. It involves writing a program or using specialized tools to parse the HTML or other structured data on a website and extract the desired information. Web scraping enables automated data collection from multiple sources on the internet without the need for manual intervention. \n",
    "\n",
    "The purposes of webscrapping are as follows:\n",
    "1. Market Research and Competitive Intelligence: Web scraping helps in monitoring competitors by extracting information from their websites, such as pricing data, product catalogs, customer reviews, and social media mentions.\n",
    "\n",
    "2. Academic Research and Data Science: Researchers and data scientists often employ web scraping to collect data for academic studies, social science research, or building datasets for machine learning models.\n",
    "\n",
    "3. Financial and Investment Analysis: Web scraping is utilized in finance and investment sectors to gather financial data, stock prices, news articles, and other relevant information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2167e",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9699d",
   "metadata": {},
   "source": [
    "ANS: The different methods used for Web Scraping are as follows:\n",
    "1. HTML Parsing: Thos method involves parsing the HTML structure of a webpage to extract relevant data. It typically involves using libraries like Beautiful Soup to navigate the HTML tree and extract desired elements based on their tags, classes, IDs, or other attributes.\n",
    "\n",
    "2. Web Scraping Frameworks and Libraries: Several programming languages have didicated web scraping frameworks and libraries that simplity the scraping process. Example of the framework in python is Scrapy. This framework provide higher-level absractions and additional features to handle various scenarios.\n",
    "\n",
    "3. API_based Scraping: Websites offer APIs that allow developres to access and retrieve data in a structured and standardized format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e4623",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e8bd",
   "metadata": {},
   "source": [
    "ANS: Bautiful Soup iis a python library that is widely used for webscraping and parsing HTML or XML documents. It provides a convenient and intuitive way to extract data from HTML or XML source code.\n",
    "\n",
    "Beautiful Soup are used for the following reasons:\n",
    "1. HTML/XML Parsing: Beautiful Soup allows developers to parse HTML or XML documents and navigate through the document tree structure. It handles poorly formed or messy HTML, making it easier to extract data from websites with inconsistent or invalid markup.\n",
    "\n",
    "2. Easy Data Extraction: Beautiful Soup provides a simple and intuitive API for extracting data from HTML elements based on tags, attributes, classes, or other criteria.\n",
    "\n",
    "3. Tag Manipulation: Beautiful Soup allows modification of HTML or XML documents by adding, modifying, or deleting tags and their attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd339c",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d451df",
   "metadata": {},
   "source": [
    "ANS: Flask is a web framework for Python that is commonly used for biilding web applications and APIs. In the context of web scraping project, Flask can be used for various purposes such as:\n",
    "1. Building a Web Interface: Flask allows you to create a web interface where users can interact with the web scraping functionality. You can design a user-friendly interface using HTML, CSS, and JavaScript, and use Flask to handle the routing, request handling, and rendering of dynamic web pages. This enables users to input scraping parameters, view results, and interact with the scraping process through a web browser.\n",
    "\n",
    "2. Creating RESTful APIs: Flask makes it easy to build APIs that expose the web scraping functionality. You can define API endpoints that accept requests with specific parameters, trigger the scraping process, and return the scraped data in a structured format (such as JSON or XML). This allows other applications or systems to integrate with your web scraping functionality programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3475c0e",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba92a02",
   "metadata": {},
   "source": [
    "ANS: The AWS services used in the project are Code Pipeline and ElastiC Beanstalk. \n",
    "Their uses are as follows:\n",
    "\n",
    "a) Code Pipeline:\n",
    "1. Source Control Integration: It can automatically detect changes in the source code repository and trigger the pipeline.\n",
    "2. Build and Test Automation: It enables running automated tests, ensuring the code quality and correctness before deployment.\n",
    "3. Deployment Automation: It provides flexibility in configuring deployment strategies, such as rolling deployments or blue-green deployments.\n",
    "4. Visual Monitoring and Notifications: It supports notifications through Amazon SNS, Slack, or other means, providing real-time updates on pipeline execution status.\n",
    "\n",
    "b) ElastiC Beanstalk:\n",
    "1. Application Deployment: It abstracts the underlying infrastructure, including EC2 instances, load balancers, and databases, making it easier to focus on application development.\n",
    "\n",
    "2. Scalability and Load Balancing: It provisions additional instances or adjusts the resources based on the specified scaling policies. It also provides built-in load balancing capabilities to distribute traffic across multiple instances.\n",
    "3. Environment Configuration:  It enables easy management of application versions, environment variables, and settings for each environment.\n",
    "4. Easy Updates and Rollbacks: It supports zero-downtime deployments and allows easy rollbacks in case of issues or errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
