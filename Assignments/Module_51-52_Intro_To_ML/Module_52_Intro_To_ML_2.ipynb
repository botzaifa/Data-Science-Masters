{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb2f68a",
   "metadata": {},
   "source": [
    "# Assignment (16th March) : Introduction to Machine Learning - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891d0f0",
   "metadata": {},
   "source": [
    "### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269101b",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "**`1. Overfitting:`** A model learns the training data too well, including noise and outliers, resulting in poor generalization to new data.\n",
    "\n",
    "- **Consequences:** High accuracy on training data, low accuracy on test data.\n",
    "\n",
    "- **Mitigation:** Use techniques like cross-validation, regularization, pruning, or more training data.\n",
    "\n",
    "\n",
    "**`2. Underfitting:`** A model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "- **Consequences:** Poor performance on both training and test data.\n",
    "\n",
    "- **Mitigation:** Use more complex models, feature engineering, or reduce regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a06195",
   "metadata": {},
   "source": [
    "### Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0bb5b",
   "metadata": {},
   "source": [
    "**ANS:** We can reduce overfitting by:\n",
    "\n",
    "1. **Cross-validation:** Splitting data into training and validation sets to ensure the model generalizes well.\n",
    "2. **Regularization:** Adding penalties for larger coefficients (e.g., L1 or L2 regularization).\n",
    "3. **Pruning:** Simplifying decision trees by removing less important nodes.\n",
    "4. **More training data:** Increasing the size of the training dataset.\n",
    "5. **Dropout:** Randomly dropping neurons during training in neural networks.\n",
    "6. **Early stopping:** Halting training when performance on a validation set starts to deteriorate.\n",
    "7. **Data augmentation:** Increasing the diversity of the training data using techniques like rotation, scaling, and flipping in image data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733fd8a",
   "metadata": {},
   "source": [
    "### Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e810d9",
   "metadata": {},
   "source": [
    "**ANS:*** `Underfitting` occurs when a machine learning model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test datasets.\n",
    "\n",
    "**Scenarios where underfitting can occur:**\n",
    "1. **Using a linear model for non-linear data:** Applying linear regression to a dataset with complex, non-linear relationships.\n",
    "2. **Insufficient training time:** Stopping the training process too early.\n",
    "3. **High regularization:** Over-penalizing the model's complexity, restricting it from learning the data's structure.\n",
    "4. **Too few features:** Not including enough relevant features in the training data.\n",
    "5. **Incorrect model selection:** Choosing a model that is too simplistic for the problem at hand (e.g., using k-NN with k too high)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2720123",
   "metadata": {},
   "source": [
    "### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1309bb",
   "metadata": {},
   "source": [
    "**ANS:** **`Bias-Variance Tradeoff:`**\n",
    "\n",
    "- **Bias:** The error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias leads to underfitting.\n",
    "  \n",
    "- **Variance:** The error introduced by the model's sensitivity to small fluctuations in the training set. High variance leads to overfitting.\n",
    "\n",
    "**`Relationship:`**\n",
    "\n",
    "- **High Bias, Low Variance:** The model is too simple, doesn't capture the complexity of the data, and performs poorly on both training and test data.\n",
    "- **Low Bias, High Variance:** The model is too complex, captures noise in the training data, and performs well on training data but poorly on test data.\n",
    "- **Optimal Model:** A balance between bias and variance, achieving good performance on both training and test data.\n",
    "\n",
    "**`Effect on Model Performance:`**\n",
    "\n",
    "- **High Bias:** Leads to underfitting, where the model misses important trends (high error on training and test data).\n",
    "- **High Variance:** Leads to overfitting, where the model captures noise along with the trend (low error on training data, high error on test data).\n",
    "- **Balanced Bias and Variance:** The model generalizes well to new data, minimizing overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf7faf",
   "metadata": {},
   "source": [
    "### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4503a79",
   "metadata": {},
   "source": [
    "**ANS:** **`Common Methods for Detecting Overfitting and Underfitting:`**\n",
    "\n",
    "1. **Training vs. Validation Performance:**\n",
    "   - **Overfitting:** High accuracy on training data, low accuracy on validation/test data.\n",
    "   - **Underfitting:** Low accuracy on both training and validation/test data.\n",
    "\n",
    "2. **Learning Curves:**\n",
    "   - Plot training and validation errors over epochs.\n",
    "   - **Overfitting:** Training error decreases, but validation error increases after a point.\n",
    "   - **Underfitting:** Both training and validation errors are high and do not decrease significantly.\n",
    "\n",
    "3. **Cross-Validation:**\n",
    "   - Use k-fold cross-validation to assess model performance.\n",
    "   - **Overfitting:** Large differences in performance across different folds.\n",
    "   - **Underfitting:** Consistently poor performance across all folds.\n",
    "\n",
    "4. **Residual Plots:**\n",
    "   - Analyze residuals (differences between predicted and actual values).\n",
    "   - **Overfitting:** Residuals show patterns, indicating the model is capturing noise.\n",
    "   - **Underfitting:** Residuals are large and do not show any particular pattern.\n",
    "\n",
    "5. **Complexity Curves:**\n",
    "   - Plot model complexity (e.g., number of features, depth of tree) vs. performance.\n",
    "   - **Overfitting:** Performance improves on training data but worsens on validation data as complexity increases.\n",
    "   - **Underfitting:** Performance is poor on both training and validation data regardless of complexity.\n",
    "\n",
    "**`Determining Overfitting vs. Underfitting:`**\n",
    "\n",
    "- **Overfitting:** If the model performs well on training data but poorly on validation/test data.\n",
    "- **Underfitting:** If the model performs poorly on both training and validation/test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd2d93",
   "metadata": {},
   "source": [
    "### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f8dff",
   "metadata": {},
   "source": [
    "**ANS:** **`Bias:`**\n",
    "\n",
    "- **Definition:** Bias is the error introduced by approximating a complex problem with a simplified model.\n",
    "- **Characteristics:** \n",
    "  - High bias models are too simple.\n",
    "  - They fail to capture the underlying patterns in the data.\n",
    "  - Result in systematic errors.\n",
    "\n",
    "**`Variance:`**\n",
    "\n",
    "- **Definition:** Variance is the error introduced by the model's sensitivity to small fluctuations in the training data.\n",
    "- **Characteristics:** \n",
    "  - High variance models are too complex.\n",
    "  - They capture noise along with the underlying patterns.\n",
    "  - Result in high variability in model predictions.\n",
    "\n",
    "**`Examples:`**\n",
    "\n",
    "- **High Bias Models:** \n",
    "  - **Linear Regression on Non-Linear Data:** A linear regression model applied to data with a non-linear relationship will underfit.\n",
    "  - **Simple Decision Tree (Shallow Tree):** A decision tree with very few levels may not capture the complexity of the data.\n",
    "\n",
    "- **High Variance Models:** \n",
    "  - **Overly Complex Decision Tree:** A decision tree with many levels will overfit the training data.\n",
    "  - **k-Nearest Neighbors (k-NN) with k=1:** A k-NN model with k=1 will be too sensitive to noise in the training data.\n",
    "\n",
    "**`Performance Differences:`**\n",
    "\n",
    "- **High Bias Models:** \n",
    "  - **Training Performance:** Poor (high error).\n",
    "  - **Validation/Test Performance:** Poor (high error).\n",
    "  - **Example:** Linear regression on complex data shows consistently high error.\n",
    "\n",
    "- **High Variance Models:** \n",
    "  - **Training Performance:** Good (low error).\n",
    "  - **Validation/Test Performance:** Poor (high error).\n",
    "  - **Example:** Complex decision tree performs well on training data but poorly on new data due to overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60ce91",
   "metadata": {},
   "source": [
    "### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62dfa3d",
   "metadata": {},
   "source": [
    "**ANS:** `Regularization` is a technique used to prevent overfitting by adding a penalty term to the model's loss function. This penalty discourages the model from becoming too complex, thus improving its ability to generalize to new data.\n",
    "\n",
    "**`Common Regularization Techniques:`**\n",
    "\n",
    "1. **L1 Regularization (Lasso):**\n",
    "   - **Description:** Adds the absolute value of the coefficients as a penalty term to the loss function.\n",
    "   - **Mathematical Form:** \\( \\text{Loss} + \\lambda \\sum |w_i| \\)\n",
    "   - **Effect:** Encourages sparsity in the model (many coefficients become zero), effectively performing feature selection.\n",
    "\n",
    "2. **L2 Regularization (Ridge):**\n",
    "   - **Description:** Adds the square of the coefficients as a penalty term to the loss function.\n",
    "   - **Mathematical Form:** \\( \\text{Loss} + \\lambda \\sum w_i^2 \\)\n",
    "   - **Effect:** Shrinks the coefficients towards zero but rarely makes them exactly zero, helping to reduce the model complexity.\n",
    "\n",
    "3. **Dropout (for Neural Networks):**\n",
    "   - **Description:** Randomly drops a fraction of the neurons during training.\n",
    "   - **Effect:** Prevents neurons from co-adapting too much, encouraging the network to generalize better.\n",
    "\n",
    "4. **Early Stopping:**\n",
    "   - **Description:** Monitors the model's performance on a validation set and stops training when performance starts to degrade.\n",
    "   - **Effect:** Prevents the model from overfitting by halting training at the optimal point.\n",
    "\n",
    "**`How Regularization Works:`**\n",
    "\n",
    "Regularization techniques add a complexity penalty to the loss function, which the model tries to minimize along with the prediction error. This penalty discourages the model from fitting the training data too closely by either shrinking the coefficients (L1 and L2 regularization) or making the model less reliant on any single feature or neuron (Dropout). By doing so, regularization helps the model maintain a balance between bias and variance, improving its performance on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
