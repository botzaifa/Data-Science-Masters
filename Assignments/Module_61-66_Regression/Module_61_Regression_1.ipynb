{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59994b92",
   "metadata": {},
   "source": [
    "# Assignment (26th March) : Regression - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a648e7c",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13e116",
   "metadata": {},
   "source": [
    "**ANS:** **`Simple Linear Regression:`**\n",
    "\n",
    "- **Definition:** Models the relationship between two variables: one independent variable (predictor) and one dependent variable (response).\n",
    "- **Equation:** \\( y = b_0 + b_1x \\)\n",
    "  - \\( y \\): Dependent variable\n",
    "  - \\( x \\): Independent variable\n",
    "  - \\( b_0 \\): Intercept\n",
    "  - \\( b_1 \\): Slope\n",
    "\n",
    "**Example:**\n",
    "Predicting a person's weight based on their height.\n",
    "\\[ \\text{Weight} = b_0 + b_1 \\times \\text{Height} \\]\n",
    "\n",
    "**`Multiple Linear Regression:`**\n",
    "\n",
    "- **Definition:** Models the relationship between one dependent variable and two or more independent variables.\n",
    "- **Equation:** <p align = \"centre\"> \\[ y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n \\] </p>\n",
    "  - \\( y \\): Dependent variable\n",
    "  - \\( x_1, x_2, \\ldots, x_n \\): Independent variables\n",
    "  - \\( b_0 \\): Intercept\n",
    "  - \\( b_1, b_2, \\ldots, b_n \\): Coefficients\n",
    "\n",
    "**Example:**\n",
    "Predicting a person's weight based on their height and age.\n",
    "<p align = \"centre\">\n",
    "\\[ \\text{Weight} = b_0 + b_1 \\times \\text{Height} + b_2 \\times \\text{Age} \\]\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e911c",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a086d",
   "metadata": {},
   "source": [
    "**ANS** **`Assumptions of Linear Regression:`**\n",
    "\n",
    "1. **Linearity:** The relationship between the independent and dependent variables should be linear.\n",
    "2. **Independence:** Observations should be independent of each other.\n",
    "3. **Homoscedasticity:** The residuals (errors) have constant variance at every level of the independent variable.\n",
    "4. **Normality:** The residuals of the model should be normally distributed.\n",
    "5. **No Multicollinearity:** Independent variables should not be highly correlated with each other.\n",
    "\n",
    "**`Checking Assumptions:`**\n",
    "\n",
    "1. **Linearity:**\n",
    "   - **Method:** Plot the observed vs. predicted values.\n",
    "   - **Check:** Look for a linear relationship.\n",
    "\n",
    "\n",
    "2. **Independence:**\n",
    "   - **Method:** Use the Durbin-Watson test.\n",
    "   - **Check:** Values close to 2 indicate independence.\n",
    "\n",
    "\n",
    "3. **Homoscedasticity:**\n",
    "   - **Method:** Plot residuals vs. predicted values.\n",
    "   - **Check:** Look for a random scatter (no pattern).\n",
    "\n",
    "\n",
    "4. **Normality:**\n",
    "   - **Method:** Use Q-Q plot or histogram of residuals.\n",
    "   - **Check:** Residuals should follow a normal distribution.\n",
    "\n",
    "\n",
    "5. **No Multicollinearity:**\n",
    "   - **Method:** Calculate Variance Inflation Factor (VIF).\n",
    "   - **Check:** VIF values below 10 indicate low multicollinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4129d6",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e6f2f",
   "metadata": {},
   "source": [
    "**ANS:** **`Interpreting the Slope and Intercept in a Linear Regression Model:`**\n",
    "\n",
    "1. **Intercept (\\(b_0\\)):**\n",
    "   - **Definition:** The predicted value of the dependent variable when all independent variables are zero.\n",
    "   - **Interpretation:** Represents the baseline level of the dependent variable.\n",
    "\n",
    "2. **Slope (\\(b_1\\), \\(b_2\\), etc.):**\n",
    "   - **Definition:** The change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant.\n",
    "   - **Interpretation:** Indicates the strength and direction of the relationship between each independent variable and the dependent variable.\n",
    "\n",
    "**`Example: Predicting House Prices`**\n",
    "\n",
    "**Scenario:** We are predicting house prices (\\(y\\)) based on the size of the house in square feet (\\(x\\)).\n",
    "\n",
    "**Linear Regression Equation:**\n",
    "<p align = \"centre\">\n",
    "\\[ \\text{Price} = b_0 + b_1 \\times \\text{Size} \\]\n",
    "</p> \n",
    "\n",
    "Let's say we have the following estimated equation:\n",
    "<p align = \"centre\">\n",
    "\\[ \\text{Price} = 50,000 + 200 \\times \\text{Size} \\]\n",
    "</p>\n",
    "\n",
    "**`Interpretation:`**\n",
    "\n",
    "- **Intercept (50,000):** \n",
    "  - When the size of the house is 0 square feet, the predicted price is $50,000. This represents the base price of a house without considering its size (though practically, a house with 0 square feet isn't realistic, it provides a starting point for the model).\n",
    "\n",
    "- **Slope (200):**\n",
    "  - For each additional square foot of size, the house price increases by $200. This shows a positive relationship between house size and price, meaning larger houses are predicted to be more expensive.\n",
    "\n",
    "\n",
    "**`Real-World Example:`**\n",
    "\n",
    "If a house is 1,000 square feet:\n",
    "<p align = \"centre\">\n",
    "\\[ \\text{Price} = 50,000 + 200 \\times 1,000 = 50,000 + 200,000 = 250,000 \\]\n",
    "<p/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65818c12",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f578c9",
   "metadata": {},
   "source": [
    "**ANS:** Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It iteratively adjusts the model parameters to find the values that minimize the cost function, thereby improving the model's predictions.\n",
    "\n",
    "**`How Gradient Descent Works:`**\n",
    "\n",
    "1. **Initialization:** Start with initial values for the model parameters (often set randomly).\n",
    "2. **Compute Gradient:** Calculate the gradient (partial derivatives) of the cost function with respect to each parameter. This gradient indicates the direction and rate of the steepest ascent.\n",
    "3. **Update Parameters:** Adjust the parameters in the opposite direction of the gradient to move towards the minimum of the cost function. The size of the step is determined by the learning rate.\n",
    "4. **Iterate:** Repeat the process until convergence, meaning the parameters change very little or the cost function reaches a minimum value.\n",
    "\n",
    "**`Update Rule:`**\n",
    "<p align = \"centre\">\n",
    "\\[ \\theta := \\theta - \\alpha \\nabla J(\\theta) \\]\n",
    "</p>\n",
    "\n",
    "Where:\n",
    "- \\( theta \\) represents the model parameters.\n",
    "\n",
    "- \\( alpha \\) is the learning rate.\n",
    "\n",
    "- \\( nabla J(theta) \\) is the gradient of the cost function with respect to \\( \\theta \\).\n",
    "\n",
    "**`Usage in Machine Learning:`**\n",
    "\n",
    "1. **Linear Regression:** Minimize the mean squared error between predicted and actual values.\n",
    "2. **Logistic Regression:** Minimize the log loss (cross-entropy) to improve classification accuracy.\n",
    "3. **Neural Networks:** Adjust weights and biases to minimize the loss function and improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12583d6",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c30551",
   "metadata": {},
   "source": [
    "**ANS:** **`Multiple Linear Regression:`**\n",
    "\n",
    "- **Model:** Models the relationship between one dependent variable and two or more independent variables.\n",
    "- **Equation:** <p align = \"centre\"> \\( y = b_0 + b_1x_1 + b_2x_2 + dots + b_nx_n \\) </p>\n",
    "  - \\( y \\): Dependent variable\n",
    "  - \\( x_1, x_2, dots, x_n \\): Independent variables\n",
    "  - \\( b_0 \\): Intercept\n",
    "  - \\( b_1, b_2, dots, b_n \\): Coefficients for each independent variable\n",
    "\n",
    "**`Difference from Simple Linear Regression:`**\n",
    "\n",
    "- **Simple Linear Regression:** Involves one independent variable and one dependent variable. \n",
    "  - **Equation:** <p align = \"centre\"> \\( y = b_0 + b_1x \\) </p>\n",
    "- **Multiple Linear Regression:** Involves multiple independent variables predicting the dependent variable. \n",
    "  - **Equation:** <p align = \"centre\"> \\( y = b_0 + b_1x_1 + b_2x_2 + dots + b_nx_n \\) </p>\n",
    "- **Complexity:** Multiple linear regression can capture the combined effect of several predictors, whereas simple linear regression captures the effect of only one predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697809a",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1491a32",
   "metadata": {},
   "source": [
    "**ANS:** Multicollinearity occurs in multiple linear regression when two or more independent variables are highly correlated, meaning they provide redundant information about the response variable. This can make it difficult to determine the individual effect of each predictor on the dependent variable and can lead to unstable estimates of regression coefficients.\n",
    "\n",
    "\n",
    "**`Detecting Multicollinearity:`**\n",
    "\n",
    "1. **Correlation Matrix:**\n",
    "   - Calculate the correlation matrix of the independent variables.\n",
    "   - High correlations (close to 1 or -1) between pairs of variables indicate potential multicollinearity.\n",
    "\n",
    "2. **Variance Inflation Factor (VIF):**\n",
    "   - Compute the VIF for each independent variable.\n",
    "   - <p align = \"centre\"> \\( \\text{VIF}_j = \\frac{1}{1 - R_j^2} \\) </p>\n",
    "     - \\( R_j^2 \\) is the coefficient of determination of the regression of \\( X_j \\) on all other predictors.\n",
    "   - VIF values greater than 10 (sometimes 5) indicate high multicollinearity.\n",
    "\n",
    "3. **Condition Number:**\n",
    "   - Compute the condition number of the feature matrix.\n",
    "   - High condition numbers (greater than 30) indicate potential multicollinearity.\n",
    "\n",
    "**`Addressing Multicollinearity:`**\n",
    "\n",
    "1. **Remove Highly Correlated Predictors:**\n",
    "   - Identify and remove one of the highly correlated variables.\n",
    "   - This can simplify the model and reduce redundancy.\n",
    "\n",
    "2. **Combine Variables:**\n",
    "   - Create a single predictor from the correlated variables through techniques like Principal Component Analysis (PCA).\n",
    "\n",
    "3. **Ridge Regression (L2 Regularization):**\n",
    "   - Apply ridge regression, which adds a penalty to the size of the coefficients.\n",
    "   - This can help reduce the variance of the estimates without removing variables.\n",
    "\n",
    "4. **Increase Sample Size:**\n",
    "   - If feasible, increase the sample size to provide more information and reduce the impact of multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a97b24e",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3346bd",
   "metadata": {},
   "source": [
    "**ANS:** **`Polynomial Regression:`**\n",
    "\n",
    "- **Model:** Extends linear regression by including polynomial terms of the independent variables.\n",
    "- **Equation:** \\( y = b_0 + b_1x + b_2x^2 + \\ldots + b_nx^n \\)\n",
    "- **Purpose:** Captures non-linear relationships between the independent and dependent variables.\n",
    "\n",
    "**`Difference from Linear Regression:`**\n",
    "\n",
    "- **Linear Regression:** Models a straight-line relationship (\\( y = b_0 + b_1x \\)).\n",
    "- **Polynomial Regression:** Models a curved relationship by including higher-order terms (\\( x^2, x^3, \\ldots \\))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84979c9d",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac58ac",
   "metadata": {},
   "source": [
    "**ANS:** **`Advantages of Polynomial Regression:`**\n",
    "\n",
    "1. **Captures Non-Linearity:** Can model complex, non-linear relationships between variables.\n",
    "2. **Flexibility:** Higher-degree polynomials can fit a wider range of data patterns.\n",
    "\n",
    "**`Disadvantages of Polynomial Regression:`**\n",
    "\n",
    "1. **Overfitting:** High-degree polynomials can fit the noise in the data, leading to poor generalization.\n",
    "2. **Complexity:** More difficult to interpret and require more computational resources.\n",
    "3. **Extrapolation:** Poor at predicting outside the range of the observed data.\n",
    "\n",
    "**`When to Prefer Polynomial Regression:`**\n",
    "\n",
    "- **Non-Linear Relationships:** When the relationship between variables is clearly non-linear and cannot be captured by a straight line.\n",
    "- **Data Patterns:** When exploratory data analysis shows a curved pattern that linear regression fails to fit well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
