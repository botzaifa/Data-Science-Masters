{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbb0a80",
   "metadata": {},
   "source": [
    "# Assignment (27th March) : Regression - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a570a",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6c3f5",
   "metadata": {},
   "source": [
    "**ANS:** `R-squared` (R^2) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in the model.\n",
    "- **Range:** (R^2) ranges from 0 to 1.\n",
    "  - \\(0\\): Indicates that the model explains none of the variance in the dependent variable.\n",
    "  - \\(1\\): Indicates that the model explains all the variance in the dependent variable.\n",
    "\n",
    "**`Calculation:`**\n",
    "\n",
    "<p>\n",
    "\\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\n",
    "</p>\n",
    "    \n",
    "`Where`:\n",
    "- <p> \\( SS_{res} \\) </p> (Residual Sum of Squares): Sum of squared differences between the observed values and the predicted values.\n",
    "  \\[ SS_{res} = \\sum (y_i - \\hat{y_i})^2 \\]\n",
    "- <p>\\( SS_{tot} \\)</p> (Total Sum of Squares): Sum of squared differences between the observed values and the mean of the observed values.\n",
    "  \\[ SS_{tot} = \\sum (y_i - \\bar{y})^2 \\]\n",
    "\n",
    "**`It represents:`**\n",
    "\n",
    "- **High \\(R^2\\):** Indicates that a large proportion of the variance in the dependent variable is explained by the independent variables.\n",
    "- **Low \\(R^2\\):** Indicates that the model does not explain much of the variance in the dependent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26d35b",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d818f",
   "metadata": {},
   "source": [
    "**ANS:** `Adjusted R-squared` R^2_adj is a modified version of R-squared that adjusts for the number of predictors in the model. It accounts for the model complexity by penalizing the addition of unnecessary predictors.\n",
    "- **Formula:**\n",
    "  <p> \\[ R^2_{adj} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right) \\] </p>\n",
    "  <p>\n",
    "  - \\( n \\): Number of observations\n",
    "  - \\( k \\): Number of predictors\n",
    "  - \\( R^2 \\): Regular R-squared\n",
    "  </p>  \n",
    "    \n",
    "**`Difference from Regular R-squared:`**\n",
    "\n",
    "- **Regular R-squared:** Increases with the addition of more predictors, regardless of their relevance.\n",
    "- **Adjusted R-squared:** Increases only if the new predictor improves the model more than would be expected by chance. It can decrease if a new predictor does not add sufficient explanatory power to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c40de02",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d98dff",
   "metadata": {},
   "source": [
    "**ANS:** **`Adjusted R-squared is more appropriate to use when:`**\n",
    "\n",
    "1. **Comparing Models with Different Numbers of Predictors:** When you have multiple models with varying numbers of independent variables, adjusted R-squared helps determine which model better explains the variance in the dependent variable while accounting for model complexity.\n",
    "\n",
    "\n",
    "2. **Preventing Overfitting:** Adjusted R-squared penalizes the addition of unnecessary predictors that do not improve the model significantly, thereby helping to avoid overfitting.\n",
    "\n",
    "\n",
    "3. **Evaluating Model Performance in Multiple Regression:** In multiple linear regression, where there are several independent variables, adjusted R-squared provides a more accurate measure of the model's explanatory power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882780e",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffdc0b",
   "metadata": {},
   "source": [
    "**ANS:**\n",
    "\n",
    "1. **`Mean Absolute Error (MAE):`**\n",
    "   - **Definition:** The average of the absolute differences between predicted and actual values.\n",
    "   - **Formula:**\n",
    "     <p>\\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}| \\] </p>\n",
    "   - **Interpretation:** Measures the average magnitude of errors in a set of predictions, without considering their direction. Lower MAE indicates better model performance.\n",
    "\n",
    "2. **`Mean Squared Error (MSE):`**\n",
    "   - **Definition:** The average of the squared differences between predicted and actual values.\n",
    "   - **Formula:**\n",
    "     <p> \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 \\] </p>\n",
    "   - **Interpretation:** Measures the average squared magnitude of errors. It gives more weight to larger errors. Lower MSE indicates better model performance.\n",
    "\n",
    "3. **`Root Mean Squared Error (RMSE):`**\n",
    "   - **Definition:** The square root of the mean squared error.\n",
    "   - **Formula:**\n",
    "     <p> \\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2} \\] </p>\n",
    "   - **Interpretation:** Provides an estimate of the standard deviation of the prediction errors. It is in the same units as the dependent variable. Lower RMSE indicates better model performance.\n",
    "\n",
    "**`Representation:`**\n",
    "- **MAE:** Indicates average error magnitude.\n",
    "- **MSE:** Indicates average squared error magnitude, more sensitive to large errors.\n",
    "- **RMSE:** Indicates standard deviation of errors, same units as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e7b508",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc58488",
   "metadata": {},
   "source": [
    "**ANS:** \n",
    "\n",
    "1. **`Mean Absolute Error (MAE)`:**\n",
    "\n",
    "   **Advantages:**\n",
    "   - **Simplicity:** Easy to understand and calculate.\n",
    "   - **Interpretability:** Provides a clear measure of average error magnitude.\n",
    "   - **Robustness:** Less sensitive to outliers compared to MSE and RMSE.\n",
    "\n",
    "   **Disadvantages:**\n",
    "   - **Lacks Sensitivity to Large Errors:** Does not give more weight to larger errors, which might be important in some contexts.\n",
    "\n",
    "\n",
    "\n",
    "2. **`Mean Squared Error (MSE)`:**\n",
    "\n",
    "   **Advantages:**\n",
    "   - **Sensitivity to Large Errors:** Squaring the errors penalizes larger errors more heavily, which can be useful if large errors are particularly undesirable.\n",
    "   - **Theoretical Benefits:** Commonly used in optimization problems and statistical theory.\n",
    "\n",
    "   **Disadvantages:**\n",
    "   - **Interpretability:** The result is in squared units of the dependent variable, which can be less intuitive.\n",
    "   - **Outliers Influence:** More sensitive to outliers due to squaring of errors.\n",
    "\n",
    "\n",
    "\n",
    "3. **`Root Mean Squared Error (RMSE)`:**\n",
    "\n",
    "   **Advantages:**\n",
    "   - **Interpretability:** The result is in the same units as the dependent variable, making it more interpretable than MSE.\n",
    "   - **Sensitivity to Large Errors:** Like MSE, it penalizes larger errors more heavily.\n",
    "\n",
    "   **Disadvantages:**\n",
    "   - **Outliers Influence:** Still sensitive to outliers due to the squaring of errors before taking the square root.\n",
    "   - **Complexity:** Slightly more complex to calculate and understand compared to MAE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc060e0",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa4dc4",
   "metadata": {},
   "source": [
    "**ANS:** `Lasso (Least Absolute Shrinkage and Selection Operator) regularization` adds a penalty equivalent to the absolute value of the magnitude of coefficients to the loss function.\n",
    "- **Formula:**\n",
    "  <p>\\[ \\text{Lasso Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{n} | \\beta_j | \\] </p>\n",
    "  - RSS: Residual Sum of Squares\n",
    "  - \\(\\lambda\\): Regularization parameter controlling the strength of the penalty\n",
    "\n",
    "**`Difference from Ridge Regularization:`**\n",
    "\n",
    "- **Ridge Regularization:**\n",
    "  - Adds a penalty equivalent to the square of the magnitude of coefficients.\n",
    "  - **Formula:**\n",
    "    <p>\\[ \\text{Ridge Cost Function} = \\text{RSS} + \\lambda \\sum_{j=1}^{n} \\beta_j^2 \\]</p>\n",
    "\n",
    "- **Lasso Regularization:**\n",
    "  - Can shrink some coefficients to zero, effectively performing feature selection.\n",
    "  - **Feature Selection:** Lasso tends to select a simpler model by eliminating irrelevant features.\n",
    "\n",
    "**`When to Use Lasso:`**\n",
    "- When you suspect that many of the features are irrelevant and want a model that includes only a subset of features.\n",
    "- When feature selection is desired.\n",
    "\n",
    "**`Example:`**\n",
    "- **Lasso:** Useful in high-dimensional data with many features where you expect only a few to be significant.\n",
    "- **Ridge:** Preferred when all features are expected to contribute to the outcome but you want to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8130a115",
   "metadata": {},
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ca5c8",
   "metadata": {},
   "source": [
    "**ANS:** `Regularized linear models` add a penalty to the loss function to constrain the size of the coefficients, discouraging complex models that fit the training data too closely. It helps balance the trade-off between bias and variance.\n",
    "\n",
    "**`Example:`** Predicting house prices with many features, including irrelevant ones.\n",
    "\n",
    "**1. Ridge Regularization:** \n",
    "\n",
    "<p>\\[ \\lambda \\sum_{j=1}^{n} \\beta_j^2\\]</p>\n",
    "\n",
    "- **Effect:** Reduces coefficients' size, making the model less sensitive to the noise in the training data.\n",
    "\n",
    "**2. Lasso Regularization:** \n",
    "\n",
    "<p>\\[\\lambda \\sum_{j=1}^{n} | \\beta_j |\\]</p>\n",
    "\n",
    "\n",
    "- **Effect:** Shrinks some coefficients to zero, effectively removing irrelevant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ab91f",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda3007",
   "metadata": {},
   "source": [
    "**ANS:** **`Limitations of Regularized Linear Models:`**\n",
    "\n",
    "1. **Feature Selection Dependency:**\n",
    "   - Lasso: Can arbitrarily select one among highly correlated features and ignore others, potentially leading to biased estimates.\n",
    "   - Ridge: Does not perform feature selection, which can be problematic in high-dimensional settings where many features are irrelevant.\n",
    "\n",
    "\n",
    "2. **Model Interpretability:** Regularization introduces bias to the estimates of the coefficients, which can complicate the interpretation of the model parameters.\n",
    "\n",
    "\n",
    "3. **Choice of Regularization Parameter (\\(\\lambda\\)):**\n",
    "   - The performance of regularized models heavily depends on the selection of the regularization parameter. Choosing an inappropriate \\(\\lambda\\) can lead to underfitting (too high \\(\\lambda\\)) or overfitting (too low \\(\\lambda\\)).\n",
    "   - Requires cross-validation to tune the parameter, which can be computationally expensive.\n",
    "\n",
    "\n",
    "4. **Non-linearity Handling:** Regularized linear models are still linear models and may not capture non-linear relationships well without additional transformation of features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**`Why Regularized Linear Models May Not Always Be the Best Choice:`**\n",
    "\n",
    "1. **Non-Linear Relationships:** When the relationship between predictors and the response variable is highly non-linear, models like decision trees, random forests, or neural networks may provide better performance.\n",
    "\n",
    "\n",
    "2. **Complex Interactions:** In cases where there are complex interactions between features that are not easily captured by linear models, more sophisticated models that can handle interactions natively might be preferred.\n",
    "\n",
    "\n",
    "3. **High Dimensionality with Non-Linear Dependencies:** In high-dimensional spaces with complex dependencies, regularized linear models may not capture the underlying patterns as effectively as non-linear models.\n",
    "\n",
    "\n",
    "4. **Data Distribution:** If the data distribution does not conform to the assumptions of linearity, normality, or homoscedasticity, regularized linear models might underperform compared to other types of regression models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd0fde",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d189a2",
   "metadata": {},
   "source": [
    "**ANS:** **`Comparison:`**\n",
    "\n",
    "1. **Different Metrics:**\n",
    "   - RMSE and MAE measure errors differently. RMSE penalizes larger errors more heavily because of the squaring of the residuals, while MAE treats all errors equally.\n",
    "\n",
    "\n",
    "2. **Interpretation of Metrics:**\n",
    "   - **RMSE:** Sensitive to outliers; higher RMSE indicates that larger errors are more significant.\n",
    "   - **MAE:** Provides an average magnitude of errors; less sensitive to outliers.\n",
    "\n",
    "**`What should we choose:`**\n",
    "\n",
    "**Contextual Understanding:**\n",
    "\n",
    "- If outliers are particularly problematic in your application (e.g., predicting critical medical dosages), you might prioritize RMSE to minimize large errors.\n",
    "- If you prefer a simpler interpretation and robustness to outliers, MAE might be more appropriate.\n",
    "\n",
    "**Without Additional Context:**\n",
    "\n",
    "- It's difficult to directly compare RMSE and MAE since they emphasize different aspects of error.\n",
    "- A model with a lower MAE (Model B) might be preferred if the goal is to minimize the average error.\n",
    "- Conversely, a model with a lower RMSE (Model A) might be preferred if large errors need to be penalized more.\n",
    "\n",
    "**`Limitations of Choosing Metrics:`**\n",
    "\n",
    "1. **Metric Sensitivity:**\n",
    "   - **RMSE:** More sensitive to outliers, can exaggerate the impact of a few large errors.\n",
    "   - **MAE:** Less sensitive to outliers, provides a balanced view of all errors.\n",
    "\n",
    "2. **Unit Consistency:** RMSE and MAE are not directly comparable due to their different calculations. The units of measurement are the same, but their scales differ.\n",
    "\n",
    "3. **Application Context:** The choice should be informed by the specific context of the problem. For example, financial forecasting might prioritize MAE for a balanced error approach, whereas RMSE might be prioritized in engineering applications where large errors can be critical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d823e1b",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbdaf4",
   "metadata": {},
   "source": [
    "**ANS:** **`Comparison:`**\n",
    "\n",
    "1. **Regularization Purpose:**\n",
    "   - **Ridge (Model A):** Adds a penalty equal to the sum of the squared coefficients. Helps in handling multicollinearity and keeping all features but with reduced coefficients.\n",
    "   - **Lasso (Model B):** Adds a penalty equal to the sum of the absolute values of the coefficients. Can shrink some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "\n",
    "2. **Regularization Parameter (\\(\\lambda\\)):**\n",
    "   - Higher \\(\\lambda\\) implies stronger regularization.\n",
    "   - Direct comparison of \\(\\lambda\\) values across different regularization methods (Ridge vs. Lasso) isn't straightforward due to different effects on coefficients.\n",
    "\n",
    "\n",
    "**`Decision Criteria:`**\n",
    "\n",
    "1. **Model Performance:**\n",
    "   - Evaluate based on cross-validation scores or a common evaluation metric (e.g., RMSE, MAE) on a validation set.\n",
    "   - Check for overfitting or underfitting using these metrics.\n",
    "\n",
    "\n",
    "2. **Feature Selection Needs:**\n",
    "   - **Lasso (Model B):** Preferred if feature selection is desired or if many features are suspected to be irrelevant.\n",
    "   - **Ridge (Model A):** Preferred if all features are expected to contribute to the outcome but require shrinkage to prevent overfitting.\n",
    "\n",
    "\n",
    "**`Trade-offs and Limitations:`**\n",
    "\n",
    "1. **Interpretability:**\n",
    "   - **Lasso:** Can improve interpretability by reducing the number of features.\n",
    "   - **Ridge:** Retains all features, which may be harder to interpret if there are many irrelevant features.\n",
    "\n",
    "\n",
    "2. **Multicollinearity:**\n",
    "   - **Ridge:** More effective in dealing with multicollinearity by shrinking correlated features together.\n",
    "   - **Lasso:** May arbitrarily select one feature from a group of highly correlated features, potentially discarding useful information.\n",
    "\n",
    "\n",
    "3. **Model Complexity:**\n",
    "   - **Lasso:** Can result in simpler models with fewer non-zero coefficients.\n",
    "   - **Ridge:** Generally retains all coefficients, leading to a potentially more complex model.\n",
    "\n",
    "\n",
    "4. **Computational Efficiency:** Both methods are computationally efficient, but the choice of \\(\\lambda\\) requires cross-validation, which can be computationally intensive.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
